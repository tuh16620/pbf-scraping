{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping for the Philadelphia Bail Bond\n",
    "\n",
    "This code will scrape data from the Philadelphia Courts, cleans the data, and outputs a CSV file. Future implementation is to have it check pages on its own, but for now manual entry of end page is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    record_date = date.today()\n",
    "    last_page = get_last_page(str(record_date))\n",
    "    # This list will hold the scraped data from each page\n",
    "    scraped_list_per_page = []\n",
    "    # The current page is 1 and the end page as of now is 3 (this needs to be manually checked)\n",
    "    curr_page_num, end_page = (1, last_page)\n",
    "    # Starting at the current page and stopping at the last page of the website\n",
    "    for curr_page_num in range(end_page):\n",
    "        # Take the current page number and increament it each iteration\n",
    "        curr_page_num = 1 + curr_page_num\n",
    "        # The current webpage stores up to 24 criminal files and we are going through each page by updating the page number in the format\n",
    "        curr_page = \"https://www.courts.phila.gov/NewCriminalFilings/date/default.aspx?search={}&searchdt=&searchtype=&page={}\".format(record_date, curr_page_num)\n",
    "        # Then get the HTML file of the page as text\n",
    "        source = requests.get(curr_page).text\n",
    "        # Then create a BeautifulSoup object of the text, this makes pulling data out of HTML files easier\n",
    "        # To learn more about it read here (https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "        soup = BeautifulSoup(source)\n",
    "        # After inspecting the source code I noticed the criminal files were listed under this specific div tag\n",
    "        # The findAll function will grab each criminal file from that page\n",
    "        list_of_criminal_filings = soup.findAll(\"div\", {\"class\": \"well well-sm\"})\n",
    "        # Then pass the list of all criminal fiilings into the extract_attributes function\n",
    "        # After the extract_attributes function completes it will return a list of that whole page's scraped criminal\n",
    "        # filings and then it will continue to the next page and at the end we will have one complete joined list\n",
    "        scraped_list_per_page = (extract_attributes(list_of_criminal_filings)) + scraped_list_per_page\n",
    "    # The joined list will then be passed into the create_csv function and converted to CSV\n",
    "    create_csv(scraped_list_per_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically get the number of page numbers per record date\n",
    "def get_last_page(record_date):\n",
    "    last_page = 1\n",
    "    link = \"https://www.courts.phila.gov/NewCriminalFilings/date/default.aspx?search={}\".format(record_date)\n",
    "    source = requests.get(link).text\n",
    "    soup = BeautifulSoup(source)\n",
    "    soup_page = soup.findAll(\"ul\", {\"class\": \"pagination\"})\n",
    "    try:\n",
    "        last_page = soup_page[0].findAll(\"li\")[-2].text\n",
    "    except IndexError as error:\n",
    "        # If the date does not exist raise an error\n",
    "        soup_check = soup.findAll(\"p\", {\"class\": \"margin-top-20\"})\n",
    "        state = soup_check[0].text\n",
    "        if \"No records found.  Please try again.\" in state:\n",
    "            raise ValueError(state)\n",
    "    return int(last_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_attributes(list_of_criminal_filings):\n",
    "    list_of_criminal_file_scraped = []\n",
    "    # For each criminal file in the list of criminal filings pass it into the scrape_and_store function\n",
    "    # Then afterwards return everything to main and it will repeat this cycle for the amount of pages\n",
    "    for criminal_file in list_of_criminal_filings:\n",
    "        criminal_file_scraped = scrape_and_store(criminal_file.text)\n",
    "        list_of_criminal_file_scraped.append(criminal_file_scraped)\n",
    "    return list_of_criminal_file_scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just regex functions that helped me clean the data you can read more about regex here (https://docs.python.org/3/library/re.html)\n",
    "def scrape_and_store(text):\n",
    "    hold = text.splitlines()\n",
    "    defendant_name = re.split('Name (.*?)', hold[3])[-1]\n",
    "    age = re.split('Age (.*?)', hold[4])[-1]\n",
    "    address = hold[6]\n",
    "    city = re.split('\\t ', address.split(',')[0])[1]\n",
    "    state = re.split(\" (.*?) \", re.split(\",\", address)[1])[1]\n",
    "    zip_code = re.split(\" (.*?) \", re.split(\",\", address)[1])[2]\n",
    "    docket_number = re.split(\"Number (.*?)\", hold[11])[2]\n",
    "    filing = re.split(\" \", hold[12])\n",
    "    filing_date = filing[2]\n",
    "    filing_time = \" \".join(filing[3:5])\n",
    "    charge = re.split(\"Charge \", hold[13])[1]\n",
    "    represented = hold[15].strip()\n",
    "    in_custody = hold[16]\n",
    "    if len(in_custody) != 1:\n",
    "        try:\n",
    "            in_custody = re.split(\"Custody (.*?)\", in_custody)[2]\n",
    "        except IndexError as error:\n",
    "            in_custody = \"\"\n",
    "    bail_status = re.split(\"\\t(.*?)\", hold[-10])[-1]\n",
    "    bail_datetime = re.split(\" \", hold[-9])\n",
    "    bail_date = bail_datetime[2]\n",
    "    bail_time = \" \".join(bail_datetime[3:5])\n",
    "    bail_type = re.split(\": (.*?)\", hold[-8])[-1]\n",
    "    bail_amount = re.split(\": (.*?)\", hold[-7])[-1]\n",
    "    outstanding_bail_amt = re.split(\" \", hold[-6])[-1]\n",
    "    # Return a list of all the attributes\n",
    "    return [defendant_name, age, city, state, zip_code, docket_number, filing_date, filing_time, charge, represented, in_custody, bail_status, bail_date, bail_time, bail_type, bail_amount, outstanding_bail_amt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will make the list of lists into a CSV file with Pandas\n",
    "def create_csv(list_of_criminal_file_scraped):\n",
    "    df = pd.DataFrame(list_of_criminal_file_scraped)\n",
    "    df.to_csv(\"output.csv\", index=False, header=[\"Defendant Name\", \"Age\", \"City\", \"State\", \"Zip Code\", \"Docket Number\", \"Filing Date\", \"Filing Time\", \"Charge\", \"Represented\", \"In Custody\", \"Bail Status\", \"Bail Date\", \"Bail Time\", \"Bail Type\", \"Bail Amount\", \"Outstanding Bail Amount\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nNo records found.  Please try again.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-54b944411bee>\u001b[0m in \u001b[0;36mget_last_page\u001b[1;34m(record_date)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mlast_page\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup_page\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"li\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-972361fa1b80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-a2ae62825083>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mrecord_date\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoday\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mlast_page\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_last_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord_date\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m# This list will hold the scraped data from each page\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mscraped_list_per_page\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-54b944411bee>\u001b[0m in \u001b[0;36mget_last_page\u001b[1;34m(record_date)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup_check\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m\"No records found.  Please try again.\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_page\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: \nNo records found.  Please try again.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
